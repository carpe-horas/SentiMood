{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## í™˜ê²½ ì„¤ì • & ë¼ì´ë¸ŒëŸ¬ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key ë¡œë“œ ì„±ê³µ!\n"
     ]
    }
   ],
   "source": [
    "# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
    "import os\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .env íŒŒì¼ ë¡œë“œ (OpenAI API Key ë¶ˆëŸ¬ì˜¤ê¸°)\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if api_key:\n",
    "    print(\"âœ… OpenAI API Key ë¡œë“œ ì„±ê³µ!\")\n",
    "else:\n",
    "    print(\"âš ï¸ OpenAI API Keyê°€ ì—†ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ë§íˆ¬ ë°ì´í„°ì…‹ ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… ë§íˆ¬ ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "style_data_path = \"../../data/tone_dataset.csv\"  # ë§íˆ¬ ë°ì´í„°ì…‹ íŒŒì¼ ê²½ë¡œ\n",
    "df = pd.read_csv(style_data_path)\n",
    "\n",
    "# âœ… ëœë¤í•˜ê²Œ 5ê°œì˜ ì˜ˆì œ ì„ íƒ\n",
    "style_examples = \"\\n\".join([\n",
    "    f\"- ì›ë³¸: {row['original']}\\n  - ë³€í™˜: {row['converted']}\" \n",
    "    for _, row in df.sample(5).iterrows()\n",
    "])\n",
    "\n",
    "print(\"âœ… ë§íˆ¬ ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ!\")\n",
    "print(style_examples)  # ë¡œë“œëœ ë°ì´í„° í™•ì¸\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain í”„ë¡¬í”„íŠ¸ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… LangChain ChatPromptTemplate ì„¤ì •\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(\n",
    "        \"\"\"ë‹¹ì‹ ì€ ì²­ì†Œë…„ ê³ ë¯¼ ìƒë‹´ì„ ë„ì™€ì£¼ëŠ” AI ì±—ë´‡ì…ë‹ˆë‹¤.  \n",
    "ì‚¬ìš©ìê°€ í¸ì•ˆí•˜ê²Œ ëŒ€í™”ë¥¼ ë‚˜ëˆŒ ìˆ˜ ìˆë„ë¡, ì¹œêµ¬ì²˜ëŸ¼ ìì—°ìŠ¤ëŸ½ê²Œ ë§í•´ ì£¼ì„¸ìš”.  \n",
    "ì‘ë‹µì„ ìƒì„±í•  ë•Œ, ì•„ë˜ ìŠ¤íƒ€ì¼ ì˜ˆì œë¥¼ ì°¸ê³ í•´ì„œ ëŒ€í™” ìŠ¤íƒ€ì¼ì„ ìœ ì§€í•˜ì„¸ìš”.\n",
    "\n",
    "ğŸ’¡ **ë§íˆ¬ ìŠ¤íƒ€ì¼ ì˜ˆì œ (Few-shot ë°ì´í„°ì…‹ ê¸°ë°˜)**:  \n",
    "{style_examples}\n",
    "\n",
    "ğŸ“Œ **ì‚¬ìš©ì ì…ë ¥**:  \n",
    "\"\"\"\n",
    "    ),\n",
    "    HumanMessagePromptTemplate.from_template(\"{user_input}\")\n",
    "])\n",
    "\n",
    "print(\"âœ… LangChain í”„ë¡¬í”„íŠ¸ ì„¤ì • ì™„ë£Œ!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ë²¡í„°DB ë¶ˆëŸ¬ì˜¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversational RAG ì„¤ì • ì™„ë£Œ!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_10124\\1012020170.py:8: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True, output_key=\"answer\")\n"
     ]
    }
   ],
   "source": [
    "# âœ… ë²¡í„° DB ë¡œë“œ\n",
    "vectorstore = FAISS.load_local('../../data/db/faiss', OpenAIEmbeddings(), allow_dangerous_deserialization=True)\n",
    "\n",
    "# âœ… ëŒ€í™” ê¸°ë°˜ ê²€ìƒ‰ì„ ìœ„í•œ Retriever ì„¤ì •\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# âœ… Memory ì„¤ì • (ëŒ€í™” ì´ë ¥ ì €ì¥)\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True, output_key=\"answer\")\n",
    "\n",
    "# âœ… Conversational RAG ì„¤ì •\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0.7)\n",
    "\n",
    "conversation_rag = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    "    return_source_documents=True,\n",
    "    combine_docs_chain_kwargs={\"prompt\": prompt}  # í”„ë¡¬í”„íŠ¸ ë°˜ì˜\n",
    ")\n",
    "\n",
    "print(\"âœ… Conversational RAG ì„¤ì • ì™„ë£Œ!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG ê²€ìƒ‰ & ëŒ€í™” íë¦„ ìµœì í™” "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_rag_response(user_input):\n",
    "    \"\"\"RAG ê²€ìƒ‰ ì‹¤í–‰ í›„, ê²°ê³¼ê°€ ì ì ˆí•œì§€ íŒë‹¨\"\"\"\n",
    "    response = conversation_rag.invoke({\"question\": user_input})\n",
    "    answer = response[\"answer\"]\n",
    "\n",
    "    # âœ… ê²€ìƒ‰ ê²°ê³¼ê°€ ì˜ë¯¸ ìˆëŠ” ê²½ìš°ë§Œ ë°˜í™˜\n",
    "    if len(answer) > 5:  # ì˜ë¯¸ ìˆëŠ” ë‹µë³€ì¸ì§€ ê°„ë‹¨íˆ ì²´í¬ (ê¸¸ì´ ê¸°ì¤€)\n",
    "        return answer\n",
    "    else:\n",
    "        return None  # ì˜ë¯¸ ì—†ëŠ” ê²½ìš° RAG ê²°ê³¼ ì‚¬ìš© ì•ˆ í•¨\n",
    "\n",
    "print(\"âœ… RAG ê²€ìƒ‰ í•¨ìˆ˜ ì„¤ì • ì™„ë£Œ!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì±—ë´‡ ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_bot():\n",
    "    while True:\n",
    "        user_input = input(\"\\në‹¹ì‹ : \")\n",
    "\n",
    "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"\\nëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤. ë‹¤ìŒì— ë˜ ë§Œë‚˜ìš”.\\n\")\n",
    "            break\n",
    "\n",
    "        # âœ… í•­ìƒ RAG ì‹¤í–‰\n",
    "        rag_response = get_relevant_rag_response(user_input)\n",
    "\n",
    "        # âœ… ìµœì¢… í”„ë¡¬í”„íŠ¸ì— ë°ì´í„° ì ìš©\n",
    "        final_prompt = prompt.format(\n",
    "            style_examples=style_examples,  # ë§íˆ¬ ë°ì´í„°ì…‹ ë°˜ì˜\n",
    "            user_input=user_input\n",
    "        )\n",
    "\n",
    "        # âœ… LLM í˜¸ì¶œ (RAG ì‘ë‹µì´ ìˆìœ¼ë©´ í¬í•¨)\n",
    "        if rag_response:\n",
    "            final_prompt += f\"\\nğŸ“‚ ì°¸ê³ í•  ìƒë‹´ ì‚¬ë¡€:\\n{rag_response}\"\n",
    "\n",
    "        # âœ… LLMì´ ìµœì¢… ì‘ë‹µ ìƒì„±\n",
    "        final_answer = llm.invoke(final_prompt)\n",
    "\n",
    "        print(\"\\nì±—ë´‡:\\n\", final_answer)\n",
    "\n",
    "print(\"âœ… ì±—ë´‡ ì‹¤í–‰ ì¤€ë¹„ ì™„ë£Œ! 'chat_with_bot()' ì‹¤í–‰í•˜ë©´ ëŒ€í™” ì‹œì‘!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¬ ê³ ë¯¼ ìƒë‹´ ì±—ë´‡ ì‹œì‘! (ì¢…ë£ŒëŠ”ëŠ” 'exit' ì…ë ¥)\n",
      "ğŸ‘¤ ì‚¬ìš©ì: ë‚˜ ë„ˆë¬´ í˜ë“¤ì–´\n",
      "\n",
      "ğŸ“‚ RAG ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ ì¤‘...\n",
      "ğŸ¤– ì±—ë´‡: \"ìŒ, ìš”ì¦˜ ë§ì´ í˜ë“  ê²ƒ ê°™êµ¬ë‚˜. ë‹¤ë¥¸ ì‚¬ëŒë“¤ì€ ì˜ ì§€ë‚´ëŠ” ê²ƒ ê°™ì€ë°, ë„ˆë§Œ í˜ë“  ê¸°ë¶„ì´ ë“œëŠ” ê±´ ì •ë§ ì†ìƒí•  ê²ƒ ê°™ì•„. ê·¸ëŸ´ ë•ŒëŠ” ì •ë§ ì™¸ë¡­ê³  í˜ë“¤ê² ì§€. ìµœê·¼ì— ì–´ë–¤ ì¼ì´ ìˆì–´ì„œ ê·¸ë ‡ê²Œ ëŠë¼ê²Œ ë˜ì—ˆì–´?\"\n",
      "ğŸ‘¤ ì‚¬ìš©ì: ì• ë“¤ì´ ë‚˜ë§Œ ë¹¼ê³  ë°¥ ë¨¹ìœ¼ëŸ¬ ê°€.\n",
      "\n",
      "ğŸ“‚ RAG ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ ì¤‘...\n",
      "ğŸ¤– ì±—ë´‡: \"ì•„, ê·¸ëŸ° ì¼ì´ ìˆì—ˆêµ¬ë‚˜. ì¹œêµ¬ë“¤ì´ ë‚˜ë§Œ ë¹¼ê³  ë°¥ ë¨¹ìœ¼ëŸ¬ ê°€ë©´ ì •ë§ ì†ìƒí•˜ê³  í˜¼ì ë‚¨ê²¨ì§„ ê¸°ë¶„ì´ ë“¤ê² ì–´. ë‚˜ë§Œ ì†Œì™¸ëœ ê²ƒ ê°™ì•„ì„œ ê¸°ë¶„ì´ ë§ì´ ë¬´ê±°ì›Œì§ˆ ê²ƒ ê°™ì•„. í˜¹ì‹œ ìµœê·¼ì— ì¹œêµ¬ë“¤ê³¼ì˜ ê´€ê³„ì—ì„œ ì–´ë–¤ ë³€í™”ê°€ ìˆì—ˆë˜ ê²ƒ ê°™ì•„? ì•„ë‹ˆë©´ ì–´ë–¤ ì¼ë¡œ ì¸í•´ ê·¸ëŸ° ê¸°ë¶„ì´ ë“¤ì—ˆëŠ”ì§€ ì´ì•¼ê¸°í•´ ì¤„ ìˆ˜ ìˆì–´?\"\n",
      "ğŸ‘¤ ì‚¬ìš©ì: ê·¸ëƒ¥ ë‚  ì§ˆíˆ¬í•˜ë‚˜ë´\n",
      "\n",
      "ğŸ“‚ RAG ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ ì¤‘...\n",
      "ğŸ¤– ì±—ë´‡: \"ê·¸ëŸ° ê¸°ë¶„ì´ ë“œëŠ”êµ¬ë‚˜.. ëˆ„êµ°ê°€ê°€ ì§ˆíˆ¬í•˜ëŠ” ê²ƒ ê°™ì•„ì„œ ë§ˆìŒì´ ë¶ˆí¸í•  ê²ƒ ê°™ì•„. ê·¸ëŸ° ìƒí™©ì€ ì •ë§ í˜ë“¤ì§€. ê·¸ ì¹œêµ¬ì™€ì˜ ê´€ê³„ëŠ” ì–´ë–¤ì§€ ê¶ê¸ˆí•´. í˜¹ì‹œ ì–´ë–¤ í–‰ë™ë“¤ì´ ê·¸ëŸ° ìƒê°ì„ í•˜ê²Œ ë§Œë“¤ì—ˆëŠ”ì§€ ì´ì•¼ê¸°í•´ì¤„ ìˆ˜ ìˆì–´?\"\n",
      "ğŸ‘¤ ì‚¬ìš©ì: exit\n",
      "ğŸ”š ìƒë‹´ ì±—ë´‡ ì¢…ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "print(\"ğŸ’¬ ê³ ë¯¼ ìƒë‹´ ì±—ë´‡ ì‹œì‘! (ì¢…ë£ŒëŠ” 'exit' ì…ë ¥)\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"ğŸ‘¤ ì‚¬ìš©ì: \")  # ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°\n",
    "    print(\"ğŸ‘¤ ì‚¬ìš©ì:\", user_input)  # ì‚¬ìš©ì ì…ë ¥ ì¦‰ì‹œ ì¶œë ¥\n",
    "    sys.stdout.flush()  # ë²„í¼ ê°•ì œ í”ŒëŸ¬ì‹œ\n",
    "\n",
    "    if user_input.lower() == \"exit\":  # 'exit' ì…ë ¥ ì‹œ ì¢…ë£Œ\n",
    "        print(\"ğŸ”š ìƒë‹´ ì±—ë´‡ ì¢…ë£Œ!\")\n",
    "        break\n",
    "\n",
    "    # âœ… ë©”ëª¨ë¦¬ì— ì €ì¥ëœ ëŒ€í™” ê¸°ë¡ ë¡œë“œ\n",
    "    chat_history = memory.load_memory_variables({})[\"chat_history\"]\n",
    "\n",
    "    # âœ… RAG ê¸°ë°˜ ì‘ë‹µ (ìë™ìœ¼ë¡œ ëŒ€í™” ê¸°ë¡ í¬í•¨)\n",
    "    response = conversation_rag.invoke({\"question\": user_input})\n",
    "\n",
    "    # âœ… ğŸ” RAG ì‘ë™ ì—¬ë¶€ í™•ì¸\n",
    "    if response[\"source_documents\"]:  # ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ìˆì„ ê²½ìš°\n",
    "        print(\"\\nğŸ“‚ RAG ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ ì¤‘...\")\n",
    "    else:  # ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ì„ ê²½ìš°\n",
    "        print(\"\\nâš ï¸ ë²¡í„°DBì— ê²€ìƒ‰ëœ ë¬¸ì„œ ì—†ìŒ. GPT ëª¨ë¸ì´ ì§ì ‘ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "    # âœ… ì±—ë´‡ ì‘ë‹µ ì¶œë ¥\n",
    "    print(\"ğŸ¤– ì±—ë´‡:\", response[\"answer\"])\n",
    "    sys.stdout.flush()  # ì±—ë´‡ ì¶œë ¥ í›„ì—ë„ í”ŒëŸ¬ì‹œ\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robotpet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
