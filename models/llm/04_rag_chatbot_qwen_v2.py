# chatbot.py

import os
import pandas as pd
import sys
from dotenv import load_dotenv
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain_openai import ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate

# .env íŒŒì¼ ë¡œë“œ (OpenAI API Key)
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")

if not api_key:
    raise ValueError("âš ï¸ OpenAI API Keyê°€ ì—†ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")

# ë§íˆ¬ ë°ì´í„°ì…‹ ë¡œë“œ
style_data_path = os.path.join(os.path.dirname(__file__), "data/tone_dataset.csv")  # ê²½ë¡œ ì¡°ì •
df = pd.read_csv(style_data_path)

# ëœë¤í•˜ê²Œ 5ê°œì˜ ì˜ˆì œ ì„ íƒ
style_examples = "\n".join([
    f"- ì›ë³¸: {row['original']}\n  - ë³€í™˜: {row['converted']}" 
    for _, row in df.sample(5).iterrows()
])

# LangChain ChatPromptTemplate ì„¤ì •
prompt = ChatPromptTemplate.from_messages([
    SystemMessagePromptTemplate.from_template(
        """ë‹¹ì‹ ì€ ì²­ì†Œë…„ ê³ ë¯¼ ìƒë‹´ì„ ë„ì™€ì£¼ëŠ” AI ì±—ë´‡ì…ë‹ˆë‹¤.  
ì‚¬ìš©ìê°€ í¸ì•ˆí•˜ê²Œ ëŒ€í™”ë¥¼ ë‚˜ëˆŒ ìˆ˜ ìˆë„ë¡, ì¹œêµ¬ì²˜ëŸ¼ ìì—°ìŠ¤ëŸ½ê²Œ ë§í•´ ì£¼ì„¸ìš”.  
ì‘ë‹µì„ ìƒì„±í•  ë•Œ, ì•„ë˜ ìŠ¤íƒ€ì¼ ì˜ˆì œë¥¼ ì°¸ê³ í•´ì„œ ëŒ€í™” ìŠ¤íƒ€ì¼ì„ ìœ ì§€í•˜ì„¸ìš”.

**ë§íˆ¬ ìŠ¤íƒ€ì¼ ì˜ˆì œ (Few-shot ë°ì´í„°ì…‹ ê¸°ë°˜)**:  
{style_examples}

**ì‚¬ìš©ì ì…ë ¥**:  
"""
    ),
    HumanMessagePromptTemplate.from_template("{user_input}")
])

# ë²¡í„° DB ë¡œë“œ
vectorstore = FAISS.load_local("data/db/faiss", OpenAIEmbeddings(), allow_dangerous_deserialization=True)

# ëŒ€í™” ê¸°ë°˜ ê²€ìƒ‰ì„ ìœ„í•œ Retriever ì„¤ì •
retriever = vectorstore.as_retriever()

# Memory ì„¤ì • (ëŒ€í™” ì´ë ¥ ì €ì¥)
memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True, output_key="answer")

# Conversational RAG ì„¤ì •
llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.7)

conversation_rag = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=retriever,
    memory=memory,
    return_source_documents=True,
    combine_docs_chain_kwargs={"prompt": prompt}  # í”„ë¡¬í”„íŠ¸ ë°˜ì˜
)

print("Conversational RAG ì„¤ì • ì™„ë£Œ!")


def get_relevant_rag_response(user_input):
    """RAG ê²€ìƒ‰ ì‹¤í–‰ í›„, ê²°ê³¼ê°€ ì ì ˆí•œì§€ íŒë‹¨"""
    response = conversation_rag.invoke({"question": user_input})
    answer = response["answer"]

    # ê²€ìƒ‰ ê²°ê³¼ê°€ ì˜ë¯¸ ìˆëŠ” ê²½ìš°ë§Œ ë°˜í™˜
    if len(answer) > 5:  # ì˜ë¯¸ ìˆëŠ” ë‹µë³€ì¸ì§€ ê°„ë‹¨íˆ ì²´í¬ (ê¸¸ì´ ê¸°ì¤€)
        return answer
    else:
        return None  # ì˜ë¯¸ ì—†ëŠ” ê²½ìš° RAG ê²°ê³¼ ì‚¬ìš© ì•ˆ í•¨


def chat_with_bot():
    """CLI ëª¨ë“œì—ì„œ ì±—ë´‡ ì‹¤í–‰"""
    print("ê³ ë¯¼ ìƒë‹´ ì±—ë´‡ ì‹œì‘! (ì¢…ë£ŒëŠ” 'exit' ì…ë ¥)")

    while True:
        user_input = input("ğŸ‘¤ ì‚¬ìš©ì: ")
        print("ğŸ‘¤ ì‚¬ìš©ì:", user_input)
        sys.stdout.flush()

        if user_input.lower() in ["exit", "quit"]:
            print("ìƒë‹´ ì±—ë´‡ ì¢…ë£Œ!")
            break

        # í•­ìƒ RAG ì‹¤í–‰
        rag_response = get_relevant_rag_response(user_input)

        # âœ… ìµœì¢… í”„ë¡¬í”„íŠ¸ì— ë°ì´í„° ì ìš©
        final_prompt = prompt.format(
            style_examples=style_examples,  # ë§íˆ¬ ë°ì´í„°ì…‹ ë°˜ì˜
            user_input=user_input
        )

        # LLM í˜¸ì¶œ (RAG ì‘ë‹µì´ ìˆìœ¼ë©´ í¬í•¨)
        if rag_response:
            final_prompt += f"\nğŸ“‚ ì°¸ê³ í•  ìƒë‹´ ì‚¬ë¡€:\n{rag_response}"

        # LLMì´ ìµœì¢… ì‘ë‹µ ìƒì„±
        final_answer = llm.invoke(final_prompt)

        print("\nğŸ¤– ì±—ë´‡:\n", final_answer)
        sys.stdout.flush()


if __name__ == "__main__":
    chat_with_bot()
